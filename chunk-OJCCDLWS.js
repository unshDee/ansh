import{a as S}from"./chunk-DBS6YHC4.js";import{a as E}from"./chunk-ARLT7E4K.js";import{a as v}from"./chunk-MBTDRT46.js";import"./chunk-CT3WOVP3.js";import"./chunk-KD5KPFK4.js";import{Ga as s,Hb as f,Ka as g,Oa as x,Xa as r,bb as i,cb as t,db as u,kb as m,lb as c,mb as d,nb as e,qb as h}from"./chunk-M4WCLFPR.js";var w=["speechMetricsChart"],b=["performanceChart"],C=()=>["BERT","Stable Diffusion v2","PyTorch","Google Cloud Speech-to-Text API","Natural Language Processing","Deep Learning"],y=class p{constructor(o){this.analyticsService=o}speechMetricsChart;performanceChart;ngOnInit(){this.analyticsService.trackProjectView("VoxArt"),this.analyticsService.trackEvent("page_view",{page_title:"VoxArt - Speech-to-Image Synthesis",page_location:"/projects/voxart",project_category:"ai-research"})}ngAfterViewInit(){}ngOnDestroy(){}static \u0275fac=function(n){return new(n||p)(g(S))};static \u0275cmp=x({type:p,selectors:[["app-voxart"]],viewQuery:function(n,l){if(n&1&&(m(w,5),m(b,5)),n&2){let a;c(a=d())&&(l.speechMetricsChart=a.first),c(a=d())&&(l.performanceChart=a.first)}},decls:311,vars:4,consts:[["projectTitle","VoxArt","note","Bachelor Thesis Project",3,"technologies"],["description",""],[1,"italic"],["content",""],[1,"grid","grid-cols-4","gap-4"],[1,"font-mono","text-sm"],[1,"font-bold"],[1,"text-xl","font-serif","text-justify","col-span-4"],[1,"text-3xl","my-4","mt-10"],[1,"font-serif","text-xl","text-justify","mb-6"],[1,"grid","grid-cols-1","gap-4","mb-8"],["windowTitle","VoxArt Processing Pipeline",1,"col-span-1"],[1,"grid","grid-cols-1","lg:grid-cols-3","gap-4"],[1,"font-bold","pb-2","mb-2","border-b-1"],[1,"text-sm","text-justify"],[1,"architecture-step"],[1,"text-3xl","my-4","mt-8"],[1,"font-serif","text-xl","text-justify","mb-4"],["windowTitle","Component Performance Analysis",1,"col-span-1"],[1,"space-y-4","grid","grid-cols-1","lg:grid-cols-2","gap-4","mb-8"],[1,"font-semibold","mb-2","pb-2","border-b-1"],[1,"font-mono","text-sm","space-y-1"],[1,"flex","justify-between"],[1,"text-xs","mt-3","text-justify","leading-relaxed"],[1,"font-bold","text-green-600"],[1,"grid","grid-cols-1","xl:grid-cols-2","gap-4","mb-8"],["windowTitle","Core Technologies",1,"col-span-1"],[1,"list-disc","pl-4","space-y-2","text-justify"],["windowTitle","Key Features",1,"col-span-1"],[1,"grid","grid-cols-1","lg:grid-cols-2","gap-4","mb-8"],["windowTitle","Dataset Characteristics",1,"col-span-1",3,"initialMinimized"],[1,"font-mono","text-sm","space-y-2"],[1,"text-sm","mt-2","text-justify"],["windowTitle","Quality Metrics",1,"col-span-1",3,"initialMinimized"],[1,"list-disc","pl-4","space-y-1","text-sm"],["windowTitle","Evaluation Results",1,"col-span-1","lg:col-span-2"],[1,"mb-2","text-sm","text-justify"],[1,"tech-spec","space-y-2"],[1,"text-center","font-bold","text-green-600"],[1,"text-xs","text-justify"],[1,"grid","grid-cols-1","xl:grid-cols-3","gap-4","mb-8","text-justify"],["windowTitle","Latency Optimization",1,"col-span-1"],[1,"mb-2"],[1,"tech-spec","text-sm"],["windowTitle","Speech Accuracy",1,"col-span-1"],["windowTitle","Visual Quality",1,"col-span-1"],["windowTitle","Novel Approaches",1,"col-span-1"],[1,"list-disc","pl-4","space-y-2"],["windowTitle","Evaluation Metrics",1,"col-span-1"],[1,"grid","grid-cols-1","lg:grid-cols-3","gap-2","mb-8"],["windowTitle","Enhanced Models",1,"col-span-1"],[1,"text-justify"],["windowTitle","Expanded Applications",1,"col-span-1"],["windowTitle","Platform Integration",1,"col-span-1"]],template:function(n,l){n&1&&(i(0,"app-project-template",0)(1,"div",1),e(2," A "),i(3,"span",2),e(4,"Speech-to-Image Synthesis"),t(),e(5," system that transforms spoken language into visual art. "),t(),i(6,"div",3)(7,"div",4)(8,"span",5),e(9,"["),i(10,"span",6),e(11,"Vo"),t(),e(12,"ice "),i(13,"span",6),e(14,"x Art"),t(),e(15,"]"),t(),u(16,"br"),i(17,"div",7),e(18," VoxArt represents a different approach to multimodal AI, seamlessly integrating speech recognition with advanced image generation models. This bachelor thesis project demonstrates the practical implementation of a voice-driven creative tool that transforms spoken descriptions into high-quality visual representations using state-of-the-art deep learning architectures. "),t()(),i(19,"h2",8),e(20,"System Architecture"),t(),i(21,"div",9),e(22," The VoxArt system employs a sophisticated three-stage pipeline that processes audio input through speech recognition, performs sentiment analysis for enhanced context understanding, and generates corresponding images using Stable Diffusion v2. "),t(),i(23,"div",10)(24,"app-window",11)(25,"div",12)(26,"div")(27,"h4",13),e(28," [1] Speech Recognition "),t(),i(29,"p",14),e(30," Google Cloud Speech-to-Text API converts spoken audio into structured text with high accuracy "),t()(),i(31,"div")(32,"h4",13),e(33," [2] Sentiment Analysis "),t(),i(34,"p",14),e(35," BERT model analyzes emotional context and enhances prompt understanding "),t()(),i(36,"div",15)(37,"h4",13),e(38,"[3] Image Generation"),t(),i(39,"p",14),e(40," Stable Diffusion v2 synthesizes high-quality images from processed text descriptions "),t()()()()(),i(41,"h2",16),e(42,"Performance Metrics"),t(),i(43,"div",17),e(44," Comprehensive evaluation of the VoxArt system demonstrates exceptional performance across all integrated components, with strong results in sentiment analysis and image generation quality. "),t(),i(45,"div")(46,"app-window",18)(47,"div",19)(48,"div")(49,"h5",20),e(50," Sentiment Analysis (BERT) "),t(),i(51,"div",21)(52,"div",22)(53,"span"),e(54,"Accuracy:"),t(),i(55,"span",6),e(56,"89.5%"),t()(),i(57,"div",22)(58,"span"),e(59,"Precision:"),t(),i(60,"span",6),e(61,"91.2%"),t()(),i(62,"div",22)(63,"span"),e(64,"Recall:"),t(),i(65,"span",6),e(66,"87.8%"),t()(),i(67,"div",22)(68,"span"),e(69,"F1 Score:"),t(),i(70,"span",6),e(71,"89.4%"),t()()(),i(72,"div",23)(73,"strong"),e(74,"Accuracy:"),t(),e(75," Overall correctness of sentiment predictions across all emotional categories. "),i(76,"strong"),e(77,"Precision:"),t(),e(78," Proportion of correctly identified sentiments among all positive predictions. "),i(79,"strong"),e(80,"Recall:"),t(),e(81," Ability to identify all relevant emotional contexts in speech input. "),i(82,"strong"),e(83,"F1 Score:"),t(),e(84," Harmonic mean balancing precision and recall for robust sentiment classification. "),t()(),i(85,"div")(86,"h5",20),e(87," Image Generation (Stable Diffusion v2) "),t(),i(88,"div",21)(89,"div",22)(90,"span"),e(91,"FID Score:"),t(),i(92,"span",24),e(93,"12.6"),t()(),i(94,"div",22)(95,"span"),e(96,"Inception Score (IS):"),t(),i(97,"span",24),e(98,"21.4"),t()(),i(99,"div",22)(100,"span"),e(101,"LPIPS:"),t(),i(102,"span",24),e(103,"0.17"),t()(),i(104,"div",22)(105,"span"),e(106,"SSIM:"),t(),i(107,"span",24),e(108,"0.81"),t()(),i(109,"div",22)(110,"span"),e(111,"CLIP Score:"),t(),i(112,"span",24),e(113,"0.32"),t()(),i(114,"div",22)(115,"span"),e(116,"Coherence (human eval.):"),t(),i(117,"span",24),e(118,"4.6/5"),t()(),i(119,"div",22)(120,"span"),e(121,"Contextual Accuracy:"),t(),i(122,"span",24),e(123,"87%"),t()()(),i(124,"div",23)(125,"strong"),e(126,"FID:"),t(),e(127," Fr\xE9chet Inception Distance measures realism by comparing feature distributions (lower is better). "),i(128,"strong"),e(129,"IS:"),t(),e(130," Inception Score evaluates image quality and diversity (higher indicates better generation). "),i(131,"strong"),e(132,"LPIPS:"),t(),e(133," Learned Perceptual Image Patch Similarity for human-like quality assessment. "),i(134,"strong"),e(135,"SSIM:"),t(),e(136," Structural Similarity Index measuring luminance, contrast, and structure preservation. "),i(137,"strong"),e(138,"CLIP Score:"),t(),e(139," Vision-language model alignment between generated images and text descriptions. "),i(140,"strong"),e(141,"Coherence:"),t(),e(142," Human evaluation of visual-semantic consistency and artistic quality. "),i(143,"strong"),e(144,"Contextual Accuracy:"),t(),e(145," Correspondence between spoken emotional context and visual output. "),t()()()()(),i(146,"h2",16),e(147,"Technical Implementation"),t(),i(148,"div",17),e(149," The implementation leverages cutting-edge technologies to create a robust and scalable speech-to-image synthesis system with real-time processing capabilities and high-quality output generation. "),t(),i(150,"div",25)(151,"app-window",26)(152,"ul",27)(153,"li")(154,"strong"),e(155,"Google Cloud Speech-to-Text API:"),t(),e(156," Advanced speech recognition with support for multiple languages and real-time processing "),t(),i(157,"li")(158,"strong"),e(159,"BERT (Bidirectional Encoder Representations):"),t(),e(160," Contextual sentiment analysis and natural language understanding "),t(),i(161,"li")(162,"strong"),e(163,"Stable Diffusion v2:"),t(),e(164," State-of-the-art text-to-image generation with high-resolution output capabilities "),t(),i(165,"li")(166,"strong"),e(167,"PyTorch:"),t(),e(168," Deep learning frameworks for model integration and optimization "),t()()(),i(169,"app-window",28)(170,"ul",27)(171,"li")(172,"strong"),e(173,"Real-time Processing:"),t(),e(174," Live audio input with immediate transcription and image generation "),t(),i(175,"li")(176,"strong"),e(177,"Multi-language Support:"),t(),e(178," Recognition and processing of various spoken languages "),t(),i(179,"li")(180,"strong"),e(181,"Sentiment-aware Generation:"),t(),e(182," Enhanced image output based on emotional context analysis "),t(),i(183,"li")(184,"strong"),e(185,"User-friendly Interface:"),t(),e(186," Intuitive voice-driven interaction without technical barriers "),t()()()(),i(187,"h2",16),e(188,"Research Dataset & Methodology"),t(),i(189,"div",17),e(190," The evaluation was conducted using a comprehensive research dataset specifically designed to test the system's performance across various speech patterns, linguistic complexities, and creative description scenarios. "),t(),i(191,"div",29)(192,"app-window",30)(193,"div",31)(194,"div",22)(195,"span"),e(196,"Total Utterances:"),t(),i(197,"span",6),e(198,"500"),t()(),i(199,"div",22)(200,"span"),e(201,"Avg. Length:"),t(),i(202,"span",6),e(203,"10.2 seconds"),t()(),i(204,"div",22)(205,"span"),e(206,"Languages:"),t(),i(207,"span",6),e(208,"Multiple"),t()(),i(209,"div",22)(210,"span"),e(211,"Speakeand GPU accelerationrs:"),t(),i(212,"span",6),e(213,"Various"),t()()(),i(214,"p",32),e(215," Performance metrics calculated based on dataset of 500 utterances recorded from various speakers with average length of 10.2 seconds. "),t()(),i(216,"app-window",33)(217,"ul",34)(218,"li"),e(219,"Transcription accuracy measurement"),t(),i(220,"li"),e(221,"Word Error Rate (WER) calculation"),t(),i(222,"li"),e(223,"Language detection accuracy"),t(),i(224,"li"),e(225,"Sentiment classification performance"),t()(),i(226,"p",32),e(227," Word Error Rate measures accuracy of transcribed words compared to ground truth. Speaker diarization represents precision of correctly identifying different speakers. "),t()(),i(228,"app-window",35)(229,"p",36),e(230," The comprehensive evaluation demonstrates the system's reliability and effectiveness across all performance dimensions, with particularly strong results in image generation quality and contextual accuracy. "),t(),i(231,"div",37)(232,"div",38),e(233," Overall System Performance: 93.7% "),t(),i(234,"div",39),e(235," Language detection accuracy indicates precision of correctly identifying language spoken in audio "),t()()()(),i(236,"h2",16),e(237,"Implementation Challenges & Solutions"),t(),i(238,"div",17),e(239," Developing VoxArt presented several technical challenges, from managing real-time processing requirements to ensuring accurate interpretation of spoken descriptions and maintaining high-quality visual output. "),t(),i(240,"div",40)(241,"app-window",41)(242,"p",42),e(243," Implementing efficient pipeline processing to minimize delay between speech input and image generation output. "),t(),i(244,"div",43)(245,"strong"),e(246,"Solution:"),t(),e(247," Asynchronous processing with optimized model loading for real-time performance. "),t()(),i(248,"app-window",44)(249,"p",42),e(250," Achieving high transcription accuracy across different speakers, accents, and environmental conditions. "),t(),i(251,"div",43)(252,"strong"),e(253,"Achievement:"),t(),e(254," 95.6% transcription accuracy with robust noise filtering and adaptive recognition algorithms. "),t()(),i(255,"app-window",45)(256,"p",42),e(257," Ensuring generated images accurately reflect the semantic content and emotional context of spoken descriptions. "),t(),i(258,"div",43)(259,"strong"),e(260,"Result:"),t(),e(261," 96.7% coherence score with enhanced prompt engineering and sentiment-aware generation. "),t()()(),i(262,"h2",16),e(263,"Research Contributions"),t(),i(264,"div",9),e(265," This thesis project contributes to the field of multimodal AI by demonstrating effective integration of speech and vision technologies, providing insights into real-time processing requirements, and establishing benchmarks for voice-driven creative applications. "),t(),i(266,"div",29)(267,"app-window",46)(268,"ul",47)(269,"li"),e(270," Integration of sentiment analysis with image generation for context-aware synthesis "),t(),i(271,"li"),e(272,"Real-time multimodal processing pipeline optimization"),t(),i(273,"li"),e(274,"Voice-driven creative interface design patterns"),t(),i(275,"li"),e(276,"User experience design for creative applications"),t()()(),i(277,"app-window",48)(278,"div")(279,"ul",47)(280,"li"),e(281,"Speech recognition accuracy assessment"),t(),i(282,"li"),e(283,"Image generation quality metrics"),t(),i(284,"li"),e(285,"User satisfaction and usability studies"),t(),i(286,"li"),e(287,"System latency and performance analysis"),t(),i(288,"li"),e(289,"Contextual accuracy measurement"),t()()()()(),i(290,"h2",16),e(291,"Future Directions"),t(),i(292,"div",17),e(293," The VoxArt project opens several avenues for future research and development, including enhanced multimodal understanding, improved generation quality, and broader applications in creative and educational domains. "),t(),i(294,"div",49)(295,"app-window",50)(296,"div",51)(297,"p"),e(298," Integration of larger, more sophisticated models for improved understanding and generation capabilities, including GPT-4 and advanced diffusion models. "),t()()(),i(299,"app-window",52)(300,"div",51)(301,"p"),e(302," Extension to educational tools, accessibility applications, and professional creative workflows including design automation and content creation platforms. "),t()()(),i(303,"app-window",53)(304,"div",51)(305,"p"),e(306," Development of web and mobile applications for broader accessibility and user adoption, with cloud-based processing and collaborative features. "),t()()()(),i(307,"h2",16),e(308,"Project Impact"),t(),i(309,"div",9),e(310," VoxArt demonstrates the potential of voice-driven creative tools and contributes to making AI-powered image generation more accessible and intuitive for users across different technical backgrounds. "),t()()()),n&2&&(r("technologies",h(3,C)),s(192),r("initialMinimized",!0),s(24),r("initialMinimized",!0))},dependencies:[f,E,v],styles:['.content-grid[_ngcontent-%COMP%]{display:grid;gap:1rem}app-window[_ngcontent-%COMP%]{transition:all .3s ease-in-out}.feature-highlight[_ngcontent-%COMP%]{border-left:4px solid var(--color-primary);padding-left:1rem;margin:1rem 0}@media (max-width: 768px){.architecture-step[_ngcontent-%COMP%]:after{content:"\\2193";right:50%;top:100%;transform:translate(50%)}}.tech-spec[_ngcontent-%COMP%]{font-family:var(--font-mono)}.fade-in-section[_ngcontent-%COMP%]{animation:_ngcontent-%COMP%_fadeInUp .6s ease-out}@keyframes _ngcontent-%COMP%_fadeInUp{0%{opacity:0;transform:translateY(30px)}to{opacity:1;transform:translateY(0)}}canvas[_ngcontent-%COMP%]{background-color:var(--color-bg);transition:all .3s ease-in-out}canvas[_ngcontent-%COMP%]:hover{transform:scale(1.02);box-shadow:0 4px 8px color-mix(in srgb,var(--color-text) 10%,transparent)}.chart-container[_ngcontent-%COMP%]{position:relative;height:clamp(200px,30vh,300px);width:100%}app-window[_ngcontent-%COMP%]:has(canvas){transition:all .3s ease-in-out}app-window[_ngcontent-%COMP%]:has(canvas):hover{transform:translateY(-2px)}.metric-value[_ngcontent-%COMP%]{font-weight:700;color:var(--color-primary)}.metric-label[_ngcontent-%COMP%]{color:var(--color-text);opacity:.8}@media (max-width: 1024px){.chart-container[_ngcontent-%COMP%]{height:clamp(180px,25vh,250px)}}@media (max-width: 768px){.chart-container[_ngcontent-%COMP%]{height:clamp(150px,20vh,200px)}}']})};export{y as VoxartComponent};
