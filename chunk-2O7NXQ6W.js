import{a as g,b as I}from"./chunk-TL3FAHS2.js";import{a as T}from"./chunk-ROAAWAGX.js";import{b as k}from"./chunk-XTUVMJIF.js";import{a as L}from"./chunk-FJ5XCANI.js";import{a as C}from"./chunk-DZX3IOKV.js";import{l as z}from"./chunk-QYKDDTVU.js";import"./chunk-PNVBHQ5N.js";import{Ba as o,Bb as M,Ca as x,Ga as n,Ka as f,Oa as b,Xa as a,bb as i,cb as e,db as l,kb as u,lb as E,mb as S,nb as t,qb as w,rb as y,tb as d,ub as c,vb as v}from"./chunk-YV7RGB6H.js";var _=["augmentationChart"],H=["posWeightChart"],A=["batchSizeChart"],U=()=>["PyTorch","segmentation_models.pytorch","timm","EfficientNet-B2","U-Net","AdamW","Albumentations"],B=class h{constructor(m,r){this.analyticsService=m;this.themeService=r;g.register(...I),M(()=>{let p=this.themeService.theme();setTimeout(()=>{this.updateChartsForTheme(p)},100)})}augmentationChart;posWeightChart;batchSizeChart;charts=[];ngOnInit(){this.analyticsService.trackProjectView("Image Segmentation with Scribble Supervision"),this.analyticsService.trackEvent("page_view",{page_title:"Image Segmentation with Scribble Supervision",page_location:"/projects/image-segmentation",project_category:"computer-vision"})}ngAfterViewInit(){this.createAugmentationChart(),this.createPosWeightChart(),this.createBatchSizeChart()}ngOnDestroy(){this.charts.forEach(m=>m.destroy())}getThemeColors(){let m=this.themeService.theme()==="dark";return{text:m?"#e0e0e0":"#252525",border:m?"#444444":"#dddddd",background:m?"#1c1c1c":"#ffffff",primary:m?"#fe6060":"#db0000",secondary:m?"#ffffff":"#000000",green:m?"#4ade80":"#15803d",orange:m?"#fb923c":"#ea580c",amber:m?"#fbbf24":"#d97706",yellow:m?"#facc15":"#eab308",blue:m?"#60a5fa":"#2563eb"}}updateChartsForTheme(m){if(this.charts.length===0)return;let r=this.getThemeColors();this.charts.forEach(p=>{p.options.plugins?.title&&(p.options.plugins.title.color=r.text),p.options.plugins?.legend?.labels&&(p.options.plugins.legend.labels.color=r.text),p.options.scales&&Object.values(p.options.scales).forEach(s=>{s&&typeof s=="object"&&("ticks"in s&&s.ticks&&(s.ticks.color=r.text),"title"in s&&s.title&&(s.title.color=r.text),"grid"in s&&s.grid&&(s.grid.color=r.border))}),p.update("none")})}createAugmentationChart(){let m=this.augmentationChart.nativeElement.getContext("2d");if(!m)return;let r=this.getThemeColors(),p={type:"bar",data:{labels:["Low Aug","Medium Aug","High Aug"],datasets:[{label:"IoU Loss",data:[.8683,.8634,.8575],backgroundColor:r.primary,borderColor:r.text,borderWidth:1},{label:"Dice Loss",data:[.8688,.8596,.8567],backgroundColor:r.amber,borderColor:r.text,borderWidth:1}]},options:{responsive:!0,maintainAspectRatio:!1,plugins:{title:{display:!1},legend:{display:!0,labels:{color:r.text,font:{size:12}}}},scales:{x:{ticks:{color:r.text},grid:{color:r.border}},y:{beginAtZero:!1,min:.84,max:.88,ticks:{color:r.text},grid:{color:r.border},title:{display:!0,text:"Mean IoU",color:r.text}}}}},s=new g(m,p);this.charts.push(s)}createPosWeightChart(){let m=this.posWeightChart.nativeElement.getContext("2d");if(!m)return;let r=this.getThemeColors(),p={type:"line",data:{labels:["1.0","2.0","3.0","3.8"],datasets:[{label:"IoU + Low Aug",data:[.8673,.8656,.872,.8683],borderColor:r.primary,backgroundColor:`${r.primary}30`,pointBackgroundColor:r.primary,pointBorderColor:r.text,tension:.3,fill:!0},{label:"Dice + Low Aug",data:[.8666,.8681,.869,.8704],borderColor:r.amber,backgroundColor:`${r.amber}30`,pointBackgroundColor:r.amber,pointBorderColor:r.text,tension:.3,fill:!0}]},options:{responsive:!0,maintainAspectRatio:!1,plugins:{title:{display:!1},legend:{display:!0,labels:{color:r.text,font:{size:12}}}},scales:{x:{ticks:{color:r.text},grid:{color:r.border},title:{display:!0,text:"Positive Weight (p_c)",color:r.text}},y:{beginAtZero:!1,min:.86,max:.88,ticks:{color:r.text},grid:{color:r.border},title:{display:!0,text:"Mean IoU",color:r.text}}}}},s=new g(m,p);this.charts.push(s)}createBatchSizeChart(){let m=this.batchSizeChart.nativeElement.getContext("2d");if(!m)return;let r=this.getThemeColors(),p={type:"bar",data:{labels:["Batch 6","Batch 8","Batch 12"],datasets:[{label:"IoU + Low Aug",data:[.8688,.8682,.8681],backgroundColor:[r.green,r.amber,r.orange],borderColor:r.text,borderWidth:1}]},options:{responsive:!0,maintainAspectRatio:!1,plugins:{title:{display:!1},legend:{display:!1}},scales:{x:{ticks:{color:r.text},grid:{color:r.border}},y:{beginAtZero:!1,min:.865,max:.87,ticks:{color:r.text},grid:{color:r.border},title:{display:!0,text:"Mean IoU",color:r.text}}}}},s=new g(m,p);this.charts.push(s)}static \u0275fac=function(r){return new(r||h)(f(T),f(z))};static \u0275cmp=b({type:h,selectors:[["app-image-segmentation"]],viewQuery:function(r,p){if(r&1&&(u(_,5),u(H,5),u(A,5)),r&2){let s;E(s=S())&&(p.augmentationChart=s.first),E(s=S())&&(p.posWeightChart=s.first),E(s=S())&&(p.batchSizeChart=s.first)}},decls:1004,vars:133,consts:[["augmentationChart",""],["posWeightChart",""],["batchSizeChart",""],["projectTitle","Image Segmentation with Scribble Supervision","note","Project - Image Segmentation",3,"technologies"],["description",""],[1,"italic"],["content",""],[1,"grid","grid-cols-4","gap-4"],[1,"font-serif","text-xl","text-justify","col-span-4"],[1,"text-4xl"],[1,"text-3xl","my-4","mt-10"],[1,"grid","grid-cols-2","gap-4","mb-8"],["windowTitle","Challenge",1,"col-span-2","lg:col-span-1",3,"initialMinimized"],["windowTitle","Approach",1,"col-span-2","lg:col-span-1",3,"initialMinimized"],[1,"grid","grid-cols-1","lg:grid-cols-2","gap-6","mb-8"],[1,"col-span-1"],[1,"font-sans","text-2xl","mb-4"],[1,"font-serif","text-lg","text-justify","mb-6"],[1,"grid","grid-cols-1","gap-3"],[1,"p-4","rounded","border","border-(--color-text)/20"],[1,"font-bold"],[1,"text-sm","text-(--color-text)"],[1,"grid","grid-cols-1","gap-1"],[1,"p-4","border","border-(--color-text)/20"],[1,"flex","items-center","gap-3"],[1,"inline-flex","items-center","justify-center","w-7","h-7","bg-(--color-text)","text-(--color-bg)","text-xs","font-bold"],[1,"font-sans","font-bold","text-lg"],[1,"font-sans","text-sm","text-(--color-text)","mt-2"],[1,"inline-flex","items-center","justify-center","w-7","h-7","rounded-full","bg-(--color-text)","text-(--color-bg)","text-xs","font-bold"],[1,"grid","grid-cols-1","lg:grid-cols-2","gap-6","mb-6"],[1,"font-serif","text-lg","text-justify"],[1,"mb-3"],[1,"font-sans","grid","grid-cols-1","gap-3"],[1,"p-3","rounded","border","border-(--color-text)/20"],[1,"font-bold","mb-1"],[1,"font-mono"],[1,"p-4","py-3","rounded","border","border-(--color-text)/20","bg-(--color-bg)"],[1,"font-sans","font-bold","mb-3"],[1,"grid","grid-cols-1","gap-0","text-sm"],[1,"flex","items-start","justify-between","gap-4"],[1,"text-(--color-text)"],[1,"equation-block","text-right",3,"innerHTML"],[1,"grid","grid-cols-1","lg:grid-cols-2","gap-4","mb-8"],["windowTitle","Intersection over Union (IoU)",1,"col-span-2","lg:col-span-1",3,"initialMinimized"],[1,"font-serif","text-lg","mb-2"],[1,"equation-block",3,"innerHTML"],[1,"font-sans","text-sm","mt-3","text-(--color-text)"],["windowTitle","Mean IoU (mIoU)",1,"col-span-2","lg:col-span-1",3,"initialMinimized"],[1,"grid","grid-cols-1","gap-6","mb-8"],["windowTitle","Step 1: Scribble Encoding & Resizing",3,"initialMinimized"],[1,"font-sans"],[1,"pb-2","border-b-1","border-(--color-text)/20"],[1,"text-lg","font-bold","mb-2",2,"color","var(--color-primary)"],[1,"text-base"],[1,"grid","grid-cols-1","lg:grid-cols-3","gap-4"],[1,"p-4"],[1,"font-bold","mb-2"],[1,"text-sm","space-y-1"],[3,"innerHTML"],[1,"text-xs","italic"],["windowTitle","Step 2: Normalization & Channel Fusion",3,"initialMinimized"],[1,"grid","grid-cols-1","lg:grid-cols-2","gap-4"],[1,"text-sm","space-y-2"],[1,"font-mono","text-xs",3,"innerHTML"],[1,"bg-(--color-bg)","p-2","rounded","border","border-(--color-text)/30"],[1,"font-mono","text-xs"],[1,"font-mono","text-xs","mt-1"],[1,"font-serif","text-lg","text-justify","mb-4"],["windowTitle","Thresholding Function",3,"initialMinimized"],[1,"lg:col-span-2"],[1,"text-(--color-text)","mt-2"],[1,"text-(--color-text)","mt-3"],[1,"p-3","border","border-(--color-text)/20","bg-(--color-bg)"],[1,"equation-block","text-center",3,"innerHTML"],[1,"text-xs","text-center","text-(--color-text)","mt-2"],["windowTitle","Region-Based Loss Functions",3,"initialMinimized"],[1,"text-xs"],["windowTitle","Composite Objective Function",3,"initialMinimized"],[1,"equation-block","text-xs",3,"innerHTML"],["windowTitle","Semantic Segmentation Augmentations",3,"initialMinimized"],[1,"pb-2"],[1,"text-sm","list-disc","pl-4","space-y-1"],[1,"text-xs","text-(--color-text)","mt-2"],["windowTitle","Synthetic Scribble Generation",3,"initialMinimized"],[1,"text-xs","italic","mt-2"],[1,"grid","grid-cols-1","lg:grid-cols-3","gap-4","mb-8"],["windowTitle","Low Augmentation",1,"col-span-1",3,"initialMinimized"],[1,"list-disc","pl-4","text-sm"],[1,"mt-3","text-xs","text-(--color-text)"],["windowTitle","Medium Augmentation",1,"col-span-1",3,"initialMinimized"],["windowTitle","High Augmentation",1,"col-span-1",3,"initialMinimized"],["windowTitle","Architecture Exploration",3,"initialMinimized"],[1,"font-bold","mb-2","text-(--color-primary)"],["windowTitle","Final Architecture Details",3,"initialMinimized"],["windowTitle","Hyperparameters",3,"initialMinimized"],[1,"text-sm","grid","grid-cols-1","gap-2"],[1,"flex","items-center","justify-between"],[1,"text-xs","italic","text-(--color-text)"],[1,"p-4","rounded","border","border-(--color-text)/20","lg:col-span-2"],[1,"text-sm","grid","grid-cols-1","sm:grid-cols-2","gap-2"],["windowTitle","Cross-Validation",1,"col-span-2","lg:col-span-1",3,"initialMinimized"],["windowTitle","Ensemble Prediction",1,"col-span-2","lg:col-span-1",3,"initialMinimized"],["windowTitle","Data Utilization",1,"col-span-2","lg:col-span-1",3,"initialMinimized"],[1,"font-sans","text-2xl","mb-4","mt-8"],["windowTitle","Test Set Performance",1,"col-span-1",3,"initialMinimized"],[1,"text-center"],[1,"text-5xl","font-bold","text-(--color-green)","mb-2"],[1,"text-sm","text-(--color-text)","mb-3"],[1,"grid","grid-cols-2","gap-2","text-xs"],[1,"text-lg","text-(--color-amber)"],["windowTitle","Cross-Validation mIoU",1,"col-span-1",3,"initialMinimized"],[1,"text-xs","mt-2","text-(--color-text)"],["windowTitle","Standard Deviation",1,"col-span-1",3,"initialMinimized"],["windowTitle","Original Image",1,"col-span-1",3,"initialMinimized"],[1,"flex","justify-center","items-center","p-2"],["alt","Original sheep image",1,"w-full","h-auto","rounded",3,"src"],[1,"text-xs","text-center","mt-2","text-(--color-text)"],["windowTitle","Sparse Scribbles",1,"col-span-1",3,"initialMinimized"],[1,"flex","justify-center","items-center","p-2","rounded"],["alt","Sparse scribble annotations",1,"w-full","h-auto",3,"src"],["windowTitle","Predicted Mask",1,"col-span-1",3,"initialMinimized"],["alt","Dense prediction from model",1,"w-full","h-auto",3,"src"],["windowTitle","Loss Function vs Augmentation",1,"col-span-1","lg:col-span-2",3,"initialMinimized"],[1,"text-xs","text-center","mt-2","text-(--color-text)","px-4","pb-2"],["windowTitle","Positive Weight Sensitivity",1,"col-span-1",3,"initialMinimized"],["windowTitle","Batch Size Impact",1,"col-span-1",3,"initialMinimized"],["windowTitle","Augmentation Impact",3,"initialMinimized"],["windowTitle","Positive Weight Sensitivity",3,"initialMinimized"],["windowTitle","Architecture Insights",1,"col-span-2","lg:col-span-1",3,"initialMinimized"],["windowTitle","Loss Design Insights",1,"col-span-2","lg:col-span-1",3,"initialMinimized"],["windowTitle","Augmentation Insights",1,"col-span-2","lg:col-span-1",3,"initialMinimized"],["windowTitle","Training Insights",1,"col-span-2","lg:col-span-1",3,"initialMinimized"],[1,"font-serif","text-xl","text-justify","mb-6"]],template:function(r,p){r&1&&(i(0,"app-project-template",3)(1,"div",4)(2,"span",5),t(3,"End-to-end binary segmentation from sparse scribbles"),e(),t(4," using an EfficientNet-B2 U-Net with region-based losses and careful augmentation for weak supervision. "),e(),i(5,"div",6)(6,"div",7)(7,"div",8)(8,"span",9),t(9,"B"),e(),t(10,"inary image segmentation\u2014dividing images into foreground and background regions at the pixel level\u2014is fundamental in computer vision with applications in medical imaging, autonomous driving, and interactive editing. This project addresses the challenge of "),i(11,"span",5),t(12,"sparse supervision"),e(),t(13,", where only limited scribbles are provided instead of dense masks. By leveraging modern deep learning architectures and carefully designed loss functions, we achieve competitive segmentation performance while dramatically reducing annotation costs. The approach combines a U-Net decoder with an EfficientNet-B2 encoder, optimized using region-based losses (Dice/IoU) blended with binary cross-entropy for stable training under weak supervision. "),e()(),i(14,"h2",10),t(15,"Project Overview"),e(),i(16,"div",11)(17,"app-window",12),t(18," Predict dense binary segmentation masks from RGB images paired with sparse scribble annotations, where only a tiny fraction of pixels are labeled. "),e(),i(19,"app-window",13),t(20," End-to-end deep learning with early fusion of scribbles, region-based composite losses, and synthetic scribble augmentation to maximize generalization from limited supervision. "),e()(),i(21,"h2",10),t(22,"Methodology Pipeline"),e(),i(23,"div",14)(24,"div",15)(25,"h3",16),t(26,"Five-Stage Workflow"),e(),i(27,"div",17),t(28," The segmentation pipeline turns sparse scribbles into dense masks via a structured five-stage process, moving from input preparation to model selection and rigorous evaluation. "),e(),i(29,"div",18)(30,"div",19)(31,"div",20),t(32,"Primary Goal"),e(),i(33,"div",21),t(34," Preserve alignment between images, scribbles, and masks while maximizing mIoU under weak supervision. "),e()(),i(35,"div",19)(36,"div",20),t(37,"Outputs"),e(),i(38,"div",21),t(39," Dense binary masks with stable calibration across folds. "),e()()()(),i(40,"div",15)(41,"div",22)(42,"div",23)(43,"div",24)(44,"span",25),t(45,"1"),e(),i(46,"div",26),t(47,"Input Encoding"),e()(),i(48,"div",27),t(49," Scribble mapping, channel fusion, resizing, normalization "),e()(),i(50,"div",19)(51,"div",24)(52,"span",28),t(53,"2"),e(),i(54,"div",26),t(55,"Architecture"),e()(),i(56,"div",27),t(57," EfficientNet-B2 encoder, U-Net decoder, scSE attention "),e()(),i(58,"div",19)(59,"div",24)(60,"span",28),t(61,"3"),e(),i(62,"div",26),t(63,"Loss Design"),e()(),i(64,"div",27),t(65," Region-based (Dice/IoU) + BCE composite with class reweighting "),e()(),i(66,"div",19)(67,"div",24)(68,"span",28),t(69,"4"),e(),i(70,"div",26),t(71,"Augmentation"),e()(),i(72,"div",27),t(73," Geometric transforms, scribble synthesis, intensity variations "),e()(),i(74,"div",19)(75,"div",24)(76,"span",28),t(77,"5"),e(),i(78,"div",26),t(79,"Evaluation"),e()(),i(80,"div",27),t(81," 5-fold CV, ensemble averaging, mIoU-based selection "),e()()()()(),i(82,"h2",10),t(83,"Problem Formulation"),e(),i(84,"div",29)(85,"div",30)(86,"div",31),t(87," We observe an RGB image and a sparse scribble mask with three label states. The goal is to predict a dense binary mask that maximizes mean Intersection over Union (mIoU) against the ground truth. "),e(),i(88,"div",32)(89,"div",33)(90,"div",34),t(91,"Label Semantics"),e(),i(92,"div",21)(93,"span",35),t(94,"0"),e(),t(95," = background, "),i(96,"span",35),t(97,"1"),e(),t(98," = foreground, "),i(99,"span",35),t(100,"\u2205"),e(),t(101," = unlabeled "),e()(),i(102,"div",33)(103,"div",34),t(104,"Optimization Target"),e(),i(105,"div",21),t(106," Maximize mIoU between prediction and ground truth over foreground and background classes. "),e()()()(),i(107,"div",36)(108,"div",37),t(109,"Mathematical Setup"),e(),i(110,"div",38)(111,"div",39)(112,"div",40),t(113,"RGB image"),e(),l(114,"div",41),d(115,"safeLatex"),e(),i(116,"div",39)(117,"div",40),t(118,"Scribble mask"),e(),l(119,"div",41),d(120,"safeLatex"),e(),i(121,"div",39)(122,"div",40),t(123,"Prediction"),e(),l(124,"div",41),d(125,"safeLatex"),e(),i(126,"div",39)(127,"div",40),t(128,"Ground truth"),e(),l(129,"div",41),d(130,"safeLatex"),e()()()(),i(131,"div",42)(132,"app-window",43)(133,"div",44),t(134," IoU measures the overlap between predicted and ground truth regions: "),e(),l(135,"div",45),d(136,"safeLatex"),i(137,"div",46),t(138," Ranges from 0 (no overlap) to 1 (perfect match) "),e()(),i(139,"app-window",47)(140,"div",44),t(141," mIoU averages IoU across foreground and background classes: "),e(),l(142,"div",45),d(143,"safeLatex"),i(144,"div",46),t(145," Provides balanced evaluation across both classes "),e()()(),i(146,"h2",10),t(147,"Input Processing & Encoding"),e(),i(148,"h3",16),t(149,"Preprocessing Pipeline"),e(),i(150,"div",17),t(151," Raw inputs undergo systematic transformations to prepare them for neural network processing, including scribble encoding, spatial resizing, channel-wise normalization, and early fusion of visual and annotation information. "),e(),i(152,"div",48)(153,"app-window",49)(154,"div",50)(155,"div",51)(156,"div",52),t(157," Mapping Sparse Annotations to Network Input "),e(),i(158,"div",53),t(159," Scribbles are encoded numerically and resized to match the network architecture constraints while preserving annotation integrity. "),e()(),i(160,"div",54)(161,"div",55)(162,"div",56),t(163,"Scribble Mapping"),e(),i(164,"div",57)(165,"div")(166,"span",20),t(167,"Background"),e(),t(168,": "),l(169,"span",58),d(170,"safeLatex"),e(),i(171,"div")(172,"span",20),t(173,"Foreground"),e(),t(174,": "),l(175,"span",58),d(176,"safeLatex"),e(),i(177,"div")(178,"span",20),t(179,"Unlabeled"),e(),t(180,": "),l(181,"span",58),d(182,"safeLatex"),e()()(),i(183,"div",55)(184,"div",56),t(185,"Spatial Resizing"),e(),i(186,"div",57)(187,"div")(188,"strong"),t(189,"Original: "),e(),l(190,"span",58),d(191,"safeLatex"),e(),i(192,"div")(193,"strong"),t(194,"Resized: "),e(),l(195,"span",58),d(196,"safeLatex"),e(),i(197,"div",59),t(198," Largest multiple of 32 below original resolution "),e()()(),i(199,"div",55)(200,"div",56),t(201,"Interpolation Methods"),e(),i(202,"div",57)(203,"div")(204,"strong"),t(205,"RGB Image:"),e(),t(206," Lanczos (preserves edges)"),e(),i(207,"div")(208,"strong"),t(209,"Masks:"),e(),t(210," Nearest-neighbor (preserves labels) "),e()()()()()(),i(211,"app-window",60)(212,"div",50)(213,"div",51)(214,"div",52),t(215," Early Fusion Strategy "),e(),i(216,"div",53),t(217," The scribble mask is concatenated as a fourth channel to the RGB image, providing immediate access to annotation information throughout the encoder hierarchy. "),e()(),i(218,"div",61)(219,"div",55)(220,"div",56),t(221,"Channel-wise Normalization"),e(),i(222,"div",62)(223,"div")(224,"strong"),t(225,"Mean:"),e(),l(226,"span",63),d(227,"safeLatex"),e(),i(228,"div")(229,"strong"),t(230,"Std:"),e(),l(231,"span",63),d(232,"safeLatex"),e(),i(233,"div",59),t(234," Computed from training set RGB channels "),e()()(),i(235,"div",55)(236,"div",56),t(237,"4-Channel Input Tensor"),e(),i(238,"div",62)(239,"div",64)(240,"div",65),t(241," Input = concat(RGB_norm, Scribble) "),e(),i(242,"div",66),t(243," Shape: [B, 4, 480, 352] "),e()(),i(244,"div",59),t(245," Enables contextual scribble interpretation "),e()()()()()()(),i(246,"h2",10),t(247,"Model Output & Thresholding"),e(),i(248,"div",42)(249,"div",15)(250,"h3",16),t(251,"Probabilistic Prediction"),e(),i(252,"div",67),t(253," The network outputs a continuous probability map "),l(254,"span",58),d(255,"safeLatex"),t(256," where each pixel value represents foreground probability. "),e()(),i(257,"div",15)(258,"h3",16),t(259,"Binary Decision"),e(),i(260,"div",67),t(261," A fixed threshold "),l(262,"span",58),d(263,"safeLatex"),t(264," converts probabilities to binary predictions without calibration. "),e()()(),i(265,"app-window",68)(266,"div",54)(267,"div",69)(268,"div",50),t(269," The binary mask is obtained by applying a fixed threshold to the probability map. "),e(),i(270,"div",70),t(271," This produces a hard decision used for evaluation and reporting. "),e(),i(272,"div",71),t(273," Gradient descent optimizes the soft probabilities "),l(274,"span",58),d(275,"safeLatex"),t(276,", not the binarized masks "),l(277,"span",58),d(278,"safeLatex"),t(279,". "),e()(),i(280,"div",72),l(281,"div",73),d(282,"safeLatex"),i(283,"div",74),l(284,"span",58),d(285,"safeLatex"),e()()()(),i(286,"h2",10),t(287,"Loss Function Design"),e(),i(288,"h3",16),t(289,"Composite Loss Strategy"),e(),i(290,"div",17),t(291," Training optimizes a weighted combination of region-based and pixel-wise objectives. Region-based losses (Dice, IoU) directly align with the mIoU evaluation metric, while binary cross-entropy provides stable gradients during early training and sparse supervision scenarios. "),e(),i(292,"div",48)(293,"app-window",75)(294,"div",50)(295,"div",51)(296,"div",52),t(297," Maximizing Overlap Metrics "),e(),i(298,"div",53),t(299," Dice and IoU losses operate on entire prediction regions, providing strong supervision signal even when pixel-level labels are sparse. "),e()(),i(300,"div",61)(301,"div",55)(302,"div",56),t(303,"Dice Loss"),e(),i(304,"div",62)(305,"div",64),l(306,"div",45),d(307,"safeLatex"),e(),i(308,"div",76)(309,"strong"),t(310,"Properties:"),e(),t(311," Numerically stable, balanced precision-recall "),e()()(),i(312,"div",55)(313,"div",56),t(314,"IoU Loss"),e(),i(315,"div",62)(316,"div",64),l(317,"div",45),d(318,"safeLatex"),e(),i(319,"div",76)(320,"strong"),t(321,"Properties:"),e(),t(322," Directly optimizes mIoU metric "),e()()()()()(),i(323,"app-window",77)(324,"div",50)(325,"div",51)(326,"div",52),t(327," Blending Region and Pixel Losses "),e(),i(328,"div",53),t(329," The final objective combines region-based terms with binary cross-entropy for improved gradient flow and training stability. "),e()(),i(330,"div",61)(331,"div",55)(332,"div",56),t(333,"Weighted Combination"),e(),i(334,"div",62)(335,"div",64),l(336,"div",45),d(337,"safeLatex"),e(),i(338,"div",76),t(339),e()()(),i(340,"div",55)(341,"div",56),t(342,"Hyperparameters"),e(),i(343,"div",62)(344,"div",64)(345,"div",65),t(346,"\u03BB_REG = 1.0"),e(),i(347,"div",66),t(348,"\u03BB_BCE = 0.2"),e()(),i(349,"div",59),t(350,"Fixed across all experiments"),e()()(),i(351,"div",55)(352,"div",56),t(353,"Class Imbalance Handling"),e(),i(354,"div",62)(355,"div")(356,"strong"),t(357,"Challenge:"),e(),t(358," 3.88\xD7 more background pixels "),e(),i(359,"div",64),l(360,"div",78),d(361,"safeLatex"),e(),i(362,"div",76),t(363," Tested "),l(364,"span",58),d(365,"safeLatex"),e()()(),i(366,"div",55)(367,"div",56),t(368,"Rationale"),e(),i(369,"div",57)(370,"div")(371,"strong"),t(372,"BCE:"),e(),t(373," Reliable pixel-level guidance"),e(),i(374,"div")(375,"strong"),t(376,"Region Loss:"),e(),t(377," Captures global structure "),e(),i(378,"div",59),t(379," Gradients from region losses can be weak when overlap is low "),e()()()()()()(),i(380,"h2",10),t(381,"Data Augmentation"),e(),i(382,"h3",16),t(383,"Augmentation Strategy"),e(),i(384,"div",17),t(385," Data augmentation increases training set diversity and prevents overfitting. We employ standard semantic segmentation transforms alongside a novel synthetic scribble generation algorithm to simulate varied annotation patterns. "),e(),i(386,"div",48)(387,"app-window",79)(388,"div",50)(389,"div",80)(390,"div",52),t(391," Standard Geometric & Photometric Transforms "),e(),i(392,"div",53),t(393," Augmentations are applied jointly to image, scribble, and mask to preserve spatial correspondence and annotation integrity. "),e()(),i(394,"div",54)(395,"div",19)(396,"div",56),t(397,"Geometric"),e(),i(398,"ul",81)(399,"li"),t(400,"Rotation"),e(),i(401,"li"),t(402,"Translation"),e(),i(403,"li"),t(404,"Scaling"),e(),i(405,"li"),t(406,"Horizontal / vertical flips"),e()(),i(407,"div",82),t(408," Maintains spatial alignment across image, scribble, and mask. "),e()(),i(409,"div",19)(410,"div",56),t(411,"Photometric"),e(),i(412,"ul",81)(413,"li"),t(414,"Color jitter"),e(),i(415,"li"),t(416,"Brightness / contrast"),e(),i(417,"li"),t(418,"Saturation"),e()(),i(419,"div",82),t(420," Applied to the RGB image only. "),e()(),i(421,"div",19)(422,"div",56),t(423,"Spatial Deformations"),e(),i(424,"ul",81)(425,"li"),t(426,"Random crop"),e(),i(427,"li"),t(428,"Elastic transform"),e(),i(429,"li"),t(430,"Grid distortion"),e()(),i(431,"div",82),t(432," Aggressive deformations used for higher augmentation settings. "),e()(),i(433,"div",19)(434,"div",56),t(435,"Filtering"),e(),i(436,"ul",81)(437,"li"),t(438,"Gaussian blur"),e()(),i(439,"div",82),t(440," Image-only smoothing to reduce high-frequency noise. "),e()(),i(441,"div",19)(442,"div",56),t(443,"Foreground-Aware"),e(),i(444,"ul",81)(445,"li"),t(446,"Random crop with foreground bias"),e()(),i(447,"div",82),t(448," Prioritizes regions containing foreground pixels. "),e()(),i(449,"div",19)(450,"div",56),t(451,"Consistency Rule"),e(),i(452,"ul",81)(453,"li"),t(454,"Same transform applied to image, scribble, and mask"),e(),i(455,"li"),t(456,"Preserves label alignment under weak supervision"),e()()()()()(),i(457,"app-window",83)(458,"div",50)(459,"div",51)(460,"div",52),t(461," Simulating Human Annotation Patterns "),e(),i(462,"div",53),t(463," An algorithmic pipeline generates realistic scribble annotations from ground truth masks, enabling training on full segmentation labels while simulating sparse supervision. "),e()(),i(464,"div",61)(465,"div",55)(466,"div",56),t(467,"Foreground Scribbles"),e(),i(468,"div",62)(469,"div")(470,"strong"),t(471,"Step 1:"),e(),t(472," Morphological closing of mask "),e(),i(473,"div")(474,"strong"),t(475,"Step 2:"),e(),t(476," Extract connected components"),e(),i(477,"div")(478,"strong"),t(479,"Step 3:"),e(),t(480," Sample random line inside each region "),e(),i(481,"div")(482,"strong"),t(483,"Step 4:"),e(),t(484," Dilate lines to match thickness "),e(),i(485,"div",84),t(486," Only components above minimum area threshold are annotated "),e()()(),i(487,"div",55)(488,"div",56),t(489,"Background Scribbles"),e(),i(490,"div",62)(491,"div")(492,"strong"),t(493,"Step 1:"),e(),t(494," Compute foreground bounding box "),e(),i(495,"div")(496,"strong"),t(497,"Step 2:"),e(),t(498," Define four background regions with margin "),e(),i(499,"div")(500,"strong"),t(501,"Step 3:"),e(),t(502," Randomly select one region (area-weighted) "),e(),i(503,"div")(504,"strong"),t(505,"Step 4:"),e(),t(506," Draw and dilate background line "),e(),i(507,"div",84),t(508," Encoded as -1 by subtracting from foreground scribbles "),e()()()()()()(),i(509,"h3",16),t(510,"Augmentation Configurations"),e(),i(511,"div",17),t(512," Three augmentation intensity levels were systematically evaluated to determine the optimal trade-off between diversity and label preservation. "),e(),i(513,"div",85)(514,"app-window",86)(515,"ul",87)(516,"li"),t(517,"Affine transforms"),e(),i(518,"li"),t(519,"Flips"),e(),i(520,"li"),t(521,"Color jitter"),e(),i(522,"li"),t(523,"Scribble augmentation"),e()(),i(524,"div",88)(525,"strong"),t(526,"Focus:"),e(),t(527," Basic geometric invariance "),e()(),i(528,"app-window",89)(529,"ul",87)(530,"li"),t(531,"Low augmentation +"),e(),i(532,"li"),t(533,"Random crop"),e()(),i(534,"div",88)(535,"strong"),t(536,"Focus:"),e(),t(537," Spatial context variation "),e()(),i(538,"app-window",90)(539,"ul",87)(540,"li"),t(541,"Medium augmentation +"),e(),i(542,"li"),t(543,"Elastic transform"),e(),i(544,"li"),t(545,"Grid distortion"),e(),i(546,"li"),t(547,"Gaussian blur"),e()(),i(548,"div",88)(549,"strong"),t(550,"Focus:"),e(),t(551," Aggressive deformation robustness "),e()()(),i(552,"h2",10),t(553,"Model Architecture"),e(),i(554,"div",42)(555,"div",15)(556,"h3",16),t(557,"Encoder Selection"),e(),i(558,"div",67),t(559," Multiple lightweight encoders were evaluated. EfficientNet-B2 emerged as the optimal choice, balancing parameter efficiency with strong feature representation capacity. "),e()(),i(560,"div",15)(561,"h3",16),t(562,"Decoder Architecture"),e(),i(563,"div",67),t(564," Standard U-Net decoder with scSE (Spatial and Channel Squeeze & Excitation) attention blocks for refined feature recalibration at multiple scales. "),e()()(),i(565,"div",48)(566,"app-window",91)(567,"div",50)(568,"div",51)(569,"div",52),t(570," Systematic Encoder Evaluation "),e(),i(571,"div",53),t(572," Initial experiments with MobileNetV3 and ResNet-18 revealed insufficient capacity for the sparse supervision regime. EfficientNet models provided superior performance through compound scaling. "),e()(),i(573,"div",54)(574,"div",55)(575,"div",56),t(576,"MobileNetV3"),e(),i(577,"div",57)(578,"div")(579,"strong"),t(580,"Pros:"),e(),t(581," Extremely lightweight"),e(),i(582,"div")(583,"strong"),t(584,"Cons:"),e(),t(585," Limited representational power"),e(),i(586,"div",59),t(587," Unable to achieve low training loss "),e()()(),i(588,"div",55)(589,"div",56),t(590,"ResNet-18"),e(),i(591,"div",57)(592,"div")(593,"strong"),t(594,"Pros:"),e(),t(595," Well-established architecture"),e(),i(596,"div")(597,"strong"),t(598,"Cons:"),e(),t(599," Suboptimal parameter efficiency "),e(),i(600,"div",59),t(601," Decent but not optimal performance "),e()()(),i(602,"div",55)(603,"div",92),t(604," EfficientNet-B2 "),e(),i(605,"div",57)(606,"div")(607,"strong"),t(608,"Pros:"),e(),t(609," Balanced efficiency & capacity"),e(),i(610,"div")(611,"strong"),t(612,"Selected:"),e(),t(613," Best validation mIoU"),e(),i(614,"div",59),t(615," Optimal trade-off for this task "),e()()()()()(),i(616,"app-window",93)(617,"div",50)(618,"div",51)(619,"div",52),t(620," EfficientNet-B2 U-Net with scSE Attention "),e(),i(621,"div",53),t(622," The architecture combines efficient compound scaling with spatial and channel attention mechanisms for precise multi-scale feature refinement. "),e()(),i(623,"div",54)(624,"div",55)(625,"div",56),t(626,"Encoder"),e(),i(627,"div",57)(628,"div")(629,"strong"),t(630,"Backbone:"),e(),t(631," EfficientNet-B2"),e(),i(632,"div")(633,"strong"),t(634,"Initialization:"),e(),t(635," From scratch"),e(),i(636,"div")(637,"strong"),t(638,"Dropout:"),e(),t(639," 0.2 per block"),e(),i(640,"div",59),t(641," Depth-5 hierarchy, 32x downsampling "),e()()(),i(642,"div",55)(643,"div",56),t(644,"Decoder"),e(),i(645,"div",57)(646,"div")(647,"strong"),t(648,"Style:"),e(),t(649," U-Net symmetric skip connections "),e(),i(650,"div")(651,"strong"),t(652,"Attention:"),e(),t(653," scSE blocks"),e(),i(654,"div",59),t(655," Concurrent spatial and channel recalibration "),e()()(),i(656,"div",55)(657,"div",56),t(658,"Implementation"),e(),i(659,"div",57)(660,"div")(661,"strong"),t(662,"Framework:"),e(),i(663,"span",65),t(664,"segmentation_models.pytorch"),e()(),i(665,"div")(666,"strong"),t(667,"Encoder Source:"),e(),i(668,"span",65),t(669,"timm"),e()()()()()()()(),i(670,"h2",10),t(671,"Training Configuration"),e(),i(672,"div",42)(673,"div",15)(674,"h3",16),t(675,"Optimization Strategy"),e(),i(676,"div",67),t(677," AdamW optimizer with cosine annealing learning rate schedule and early stopping based on validation mIoU plateau detection. "),e()(),i(678,"div",15)(679,"h3",16),t(680,"Computational Efficiency"),e(),i(681,"div",67),t(682," Mixed-precision training (bfloat16) on A100 GPUs enables larger batch sizes and faster convergence without sacrificing numerical stability. "),e()()(),i(683,"div",48)(684,"app-window",94)(685,"div",50)(686,"div",61)(687,"div",19)(688,"div",56),t(689,"Optimization"),e(),i(690,"div",95)(691,"div",96)(692,"span"),t(693,"Optimizer"),e(),i(694,"span",65),t(695,"AdamW"),e()(),i(696,"div",96)(697,"span"),t(698,"Learning rate"),e(),i(699,"span",65),t(700,"5e-3"),e()(),i(701,"div",96)(702,"span"),t(703,"Weight decay"),e(),i(704,"span",65),t(705,"1e-4"),e()()()(),i(706,"div",19)(707,"div",56),t(708,"Schedule"),e(),i(709,"div",95)(710,"div",96)(711,"span"),t(712,"Policy"),e(),i(713,"span",65),t(714,"Cosine annealing"),e()(),i(715,"div",96)(716,"span"),t(717,"Min lr"),e(),i(718,"span",65),t(719,"1e-7"),e()(),i(720,"div",96)(721,"span"),t(722,"Training length"),e(),i(723,"span",65),t(724,"8K\u201316K steps"),e()()()(),i(725,"div",19)(726,"div",56),t(727,"Batching"),e(),i(728,"div",95)(729,"div",96)(730,"span"),t(731,"Batch sizes"),e(),i(732,"span",65),t(733,"6, 8, 12"),e()(),i(734,"div",97),t(735," Evaluated across configurations "),e()()(),i(736,"div",19)(737,"div",56),t(738,"Precision & Hardware"),e(),i(739,"div",95)(740,"div",96)(741,"span"),t(742,"Precision"),e(),i(743,"span",65),t(744,"bfloat16"),e()(),i(745,"div",96)(746,"span"),t(747,"Accelerator"),e(),i(748,"span",65),t(749,"A100 GPUs"),e()()()(),i(750,"div",98)(751,"div",56),t(752,"Early Stopping"),e(),i(753,"div",99)(754,"div",96)(755,"span"),t(756,"Patience"),e(),i(757,"span",65),t(758,"60 epochs"),e()(),i(759,"div",96)(760,"span"),t(761,"Metric"),e(),i(762,"span",65),t(763,"Validation mIoU"),e()()()()()()()(),i(764,"h2",10),t(765,"Experimental Results"),e(),i(766,"h3",16),t(767,"Evaluation Methodology"),e(),i(768,"div",17),t(769," 5-fold cross-validation was employed to robustly estimate performance and select the optimal configuration. Final test predictions used an ensemble of all five fold models to reduce variance and improve generalization. "),e(),i(770,"div",85)(771,"app-window",100)(772,"div",62)(773,"div")(774,"strong"),t(775,"Folds:"),e(),t(776," 5"),e(),i(777,"div")(778,"strong"),t(779,"Strategy:"),e(),t(780," Stratified split"),e(),i(781,"div")(782,"strong"),t(783,"Selection:"),e(),t(784," Max mean mIoU across folds"),e()()(),i(785,"app-window",101)(786,"div",62)(787,"div")(788,"strong"),t(789,"Method:"),e(),t(790," Average probability maps"),e(),i(791,"div")(792,"strong"),t(793,"Models:"),e(),t(794," All 5 fold checkpoints"),e(),i(795,"div")(796,"strong"),t(797,"Threshold:"),e(),t(798," \u03C4 = 0.5"),e()()(),i(799,"app-window",102)(800,"div",62)(801,"div")(802,"strong"),t(803,"Training set:"),e(),t(804," \u2248181 images"),e(),i(805,"div")(806,"strong"),t(807,"Leakage:"),e(),t(808," None (fold-based inference)"),e(),i(809,"div")(810,"strong"),t(811,"Efficiency:"),e(),t(812," All data used for final model"),e()()()(),i(813,"h3",103),t(814,"Performance Summary"),e(),i(815,"div",85)(816,"app-window",104)(817,"div",105)(818,"div",106),t(819,"0.8569"),e(),i(820,"div",107),t(821," mIoU on first test set "),e(),i(822,"div",108)(823,"div")(824,"div",20),t(825,"Background IoU"),e(),i(826,"div",109),t(827,"0.9215"),e()(),i(828,"div")(829,"div",20),t(830,"Foreground IoU"),e(),i(831,"div",109),t(832,"0.7923"),e()()()()(),i(833,"app-window",110)(834,"div",105)(835,"div",106),t(836,"0.8758"),e(),i(837,"div",21),t(838," Mean across 5 folds (best config) "),e(),i(839,"div",111),t(840," IoU Loss + Low Aug + Batch 8 + "),l(841,"span",58),d(842,"safeLatex"),e()()(),i(843,"app-window",112)(844,"div",105)(845,"div",106),t(846," \xB10.0169 "),e(),i(847,"div",21),t(848," Fold variance (best config) "),e()()()(),i(849,"h3",103),t(850,"Segmentation Example"),e(),i(851,"div",17),t(852," Visual comparison of model predictions on a test image demonstrates the quality of dense mask recovery from sparse scribble annotations. "),e(),i(853,"div",85)(854,"app-window",113)(855,"div",114),l(856,"img",115),e(),i(857,"div",116),t(858," Input image with background and foreground objects "),e()(),i(859,"app-window",117)(860,"div",118),l(861,"img",119),e(),i(862,"div",116),t(863," Minimal supervision: only scribbles provided "),e()(),i(864,"app-window",120)(865,"div",118),l(866,"img",121),e(),i(867,"div",116),t(868," Dense mask recovered from sparse input "),e()()(),i(869,"h3",103),t(870,"Ablation Studies"),e(),i(871,"div",17),t(872," Systematic experiments across 72 configurations revealed the impact of augmentation intensity, positive class weighting, batch size, and loss function on final performance, particularly in the low-data regime. "),e(),i(873,"div",42)(874,"app-window",122)(875,"div",55),l(876,"canvas",null,0),e(),i(878,"div",123),t(879," IoU loss outperforms Dice loss across all augmentation levels. Lower augmentation yields better results for this task. "),e()(),i(880,"app-window",124)(881,"div",55),l(882,"canvas",null,1),e(),i(884,"div",123),t(885," Class weighting "),l(886,"span",58),d(887,"safeLatex"),t(888," provides optimal balance for foreground objects. "),e()(),i(889,"app-window",125)(890,"div",55),l(891,"canvas",null,2),e(),i(893,"div",123),t(894," Batch size 8 achieves best trade-off between stability and generalization. "),e()()(),i(895,"div",48)(896,"app-window",126)(897,"div",50)(898,"div",51)(899,"div",52),t(900," Effect on Validation mIoU "),e(),i(901,"div",53),t(902," Contrary to typical expectations, aggressive augmentation reduced validation mIoU in this sparse supervision setting. "),e()(),i(903,"div",54)(904,"div",55)(905,"div",56),t(906,"Observation"),e(),i(907,"div",57)(908,"div"),t(909," Higher augmentation \u2192 "),i(910,"strong"),t(911,"Lower"),e(),t(912," validation mIoU "),e(),i(913,"div",59),t(914," Averaged across loss functions and folds "),e()()(),i(915,"div",55)(916,"div",56),t(917,"Hypothesis"),e(),i(918,"div",57)(919,"div"),t(920,"Small training set (\u2248181 images)"),e(),i(921,"div"),t(922,"Challenging sparse annotation regime"),e(),i(923,"div",59),t(924," Aggressive transforms may corrupt weak supervision signal "),e()()(),i(925,"div",55)(926,"div",56),t(927,"Recommendation"),e(),i(928,"div",57)(929,"div")(930,"strong"),t(931,"Low augmentation"),e(),t(932," performed best"),e(),i(933,"div",59),t(934," Suggests careful augmentation design critical for scribble supervision "),e()()()()()(),i(935,"app-window",127)(936,"div",50)(937,"div",51)(938,"div",52),t(939," Class Imbalance Reweighting "),e(),i(940,"div",53),t(941," Tested BCE positive weights "),l(942,"span",58),d(943,"safeLatex"),t(944," to address 3.88x background-to-foreground pixel ratio. "),e()(),i(945,"div",61)(946,"div",55)(947,"div",56),t(948,"Finding"),e(),i(949,"div",62)(950,"div")(951,"strong"),t(952,"Limited effect"),e(),t(953," on validation mIoU across tested values "),e(),i(954,"div",59),t(955," Region-based losses may already handle imbalance effectively "),e()()(),i(956,"div",55)(957,"div",56),t(958,"Interpretation"),e(),i(959,"div",62)(960,"div"),t(961,"Dice and IoU are inherently robust to class imbalance"),e(),i(962,"div",76),t(963," BCE contribution (\u03BB=0.2) is secondary to region losses "),e()()()()()()(),i(964,"h2",10),t(965,"Key Findings"),e(),i(966,"div",42)(967,"app-window",128)(968,"ul",87)(969,"li"),t(970," EfficientNet-B2 provides optimal efficiency-performance trade-off "),e(),i(971,"li"),t(972,"scSE attention enhances feature refinement at minimal cost"),e(),i(973,"li"),t(974,"Early fusion of scribbles enables contextual interpretation"),e()()(),i(975,"app-window",129)(976,"ul",87)(977,"li"),t(978,"Region-based losses align well with mIoU evaluation"),e(),i(979,"li"),t(980,"BCE stabilizes training under sparse supervision"),e(),i(981,"li"),t(982,"IoU and Dice losses yield similar performance"),e()()()(),i(983,"div",42)(984,"app-window",130)(985,"ul",87)(986,"li"),t(987,"Low augmentation outperforms aggressive transforms"),e(),i(988,"li"),t(989,"Small dataset regime requires careful augmentation design"),e(),i(990,"li"),t(991,"Synthetic scribble generation adds training diversity"),e()()(),i(992,"app-window",131)(993,"ul",87)(994,"li"),t(995,"Ensemble averaging improves test set stability"),e(),i(996,"li"),t(997,"Early stopping prevents overfitting to validation set"),e(),i(998,"li"),t(999,"Mixed-precision training enables efficient experimentation"),e()()()(),i(1e3,"h2",10),t(1001,"Conclusion"),e(),i(1002,"div",132),t(1003," The final system demonstrates that carefully designed loss functions and strong encoder-decoder backbones can achieve competitive segmentation from sparse scribbles. The approach reduces annotation burden while preserving high-quality masks, making it viable for large datasets with limited labeling budgets. "),e()()()),r&2&&(a("technologies",y(132,U)),n(17),a("initialMinimized",!1),n(2),a("initialMinimized",!1),n(95),a("innerHTML",c(115,68,"b \\in \\mathbb{R}^{n_x \\times n_y \\times 3}"),o),n(5),a("innerHTML",c(120,70,"c \\in \\{0,1,\\emptyset\\}^{n_x \\times n_y}"),o),n(5),a("innerHTML",c(125,72,"\\hat{g} \\in \\{0,1\\}^{n_x \\times n_y}"),o),n(5),a("innerHTML",c(130,74,"g \\in \\{0,1\\}^{n_x \\times n_y}"),o),n(3),a("initialMinimized",!0),n(3),a("innerHTML",c(136,76,"\\text{IoU}_k = \\frac{|\\{g=k\\} \\cap \\{\\hat{g}=k\\}|}{|\\{g=k\\} \\cup \\{\\hat{g}=k\\}|}"),o),n(4),a("initialMinimized",!0),n(3),a("innerHTML",c(143,78,"\\text{mIoU} = \\frac{\\text{IoU}_{\\text{bg}} + \\text{IoU}_{\\text{fg}}}{2}"),o),n(11),a("initialMinimized",!0),n(16),a("innerHTML",v(170,80,"0 \\rightarrow -1",!1),o),n(6),a("innerHTML",v(176,83,"1 \\rightarrow +1",!1),o),n(6),a("innerHTML",v(182,86,"\\emptyset \\rightarrow 0",!1),o),n(9),a("innerHTML",v(191,89,"500 \\times 375",!1),o),n(5),a("innerHTML",v(196,92,"480 \\times 352",!1),o),n(16),a("initialMinimized",!0),n(15),a("innerHTML",c(227,95,"(0.4589, 0.4589, 0.4215)"),o),n(5),a("innerHTML",c(232,97,"(0.2618, 0.2633, 0.2822)"),o),n(23),a("innerHTML",c(255,99,"\\hat{p} \\in (0,1)^{n_x \\times n_y}"),o),n(8),a("innerHTML",c(263,101,"\\tau = 0.5"),o),n(3),a("initialMinimized",!0),n(9),a("innerHTML",v(275,103,"\\hat{p}",!1),o),n(3),a("innerHTML",v(278,106,"\\hat{g}",!1),o),n(4),a("innerHTML",c(282,109,"\\hat{g}_i = \\begin{cases}1 & \\text{if } \\hat{p}_i \\ge \\tau \\\\ 0 & \\text{otherwise}\\end{cases}"),o),n(3),a("innerHTML",v(285,111,"\\tau = 0.5",!1),o),n(9),a("initialMinimized",!0),n(13),a("innerHTML",c(307,114,"\\mathcal{L}_{\\text{Dice}} = 1 - \\frac{2 \\sum_i \\hat{p}_i g_i}{\\sum_i \\hat{p}_i + g_i}"),o),n(11),a("innerHTML",c(318,116,"\\mathcal{L}_{\\text{IoU}} = 1 - \\frac{\\sum_i \\hat{p}_i g_i}{\\sum_i \\hat{p}_i + g_i - \\hat{p}_i g_i}"),o),n(6),a("initialMinimized",!0),n(13),a("innerHTML",c(337,118,"\\mathcal{L}_{\\text{total}} = \\lambda_{\\text{REG}} \\mathcal{L}_{\\text{REG}} + \\lambda_{\\text{BCE}} \\mathcal{L}_{\\text{BCE}}"),o),n(3),w(" REG \u2208 ","{","Dice, IoU","}"," selected via cross-validation "),n(21),a("innerHTML",c(361,120,"\\mathcal{L}_{\\text{BCE}} = -\\sum_i p_c g_i \\log \\hat{p}_i + (1-g_i) \\log(1-\\hat{p}_i)"),o),n(4),a("innerHTML",c(365,122,"p_c \\in \\{1, 2, 3, 3.8\\}"),o),n(23),a("initialMinimized",!0),n(70),a("initialMinimized",!0),n(57),a("initialMinimized",!1),n(14),a("initialMinimized",!1),n(10),a("initialMinimized",!1),n(28),a("initialMinimized",!0),n(50),a("initialMinimized",!0),n(68),a("initialMinimized",!0),n(87),a("initialMinimized",!1),n(14),a("initialMinimized",!1),n(14),a("initialMinimized",!1),n(17),a("initialMinimized",!1),n(17),a("initialMinimized",!1),n(8),a("innerHTML",v(842,124,"p_c=3",!1),o),n(2),a("initialMinimized",!1),n(11),a("initialMinimized",!1),n(2),a("src","/img/2010_002232.jpg",x),n(3),a("initialMinimized",!1),n(2),a("src","/img/2010_002232.png",x),n(3),a("initialMinimized",!1),n(2),a("src","/img/prediction.jpg",x),n(8),a("initialMinimized",!1),n(6),a("initialMinimized",!1),n(6),a("innerHTML",v(887,127,"p_c=3",!1),o),n(3),a("initialMinimized",!1),n(7),a("initialMinimized",!0),n(39),a("initialMinimized",!0),n(7),a("innerHTML",c(943,130,"p_c \\in \\{1, 2, 3, 3.8\\}"),o),n(25),a("initialMinimized",!1),n(8),a("initialMinimized",!1),n(9),a("initialMinimized",!1),n(8),a("initialMinimized",!1))},dependencies:[L,C,k],styles:[".equation-block[_ngcontent-%COMP%]{font-size:.95rem}app-window[_ngcontent-%COMP%]{transition:transform .2s ease-in-out,box-shadow .2s ease-in-out}app-window[_ngcontent-%COMP%]:hover{transform:translateY(-2px)}[_nghost-%COMP%]     .rounded, [_nghost-%COMP%]     .rounded-sm, [_nghost-%COMP%]     .rounded-md, [_nghost-%COMP%]     .rounded-lg, [_nghost-%COMP%]     .rounded-xl, [_nghost-%COMP%]     .rounded-2xl, [_nghost-%COMP%]     .rounded-3xl, [_nghost-%COMP%]     .rounded-full{border-radius:0!important}[_nghost-%COMP%]     app-window, [_nghost-%COMP%]     app-window *{border-radius:0!important}canvas[_ngcontent-%COMP%]{max-height:400px;width:100%!important;height:auto!important}img[_ngcontent-%COMP%]{image-rendering:crisp-edges;image-rendering:-webkit-optimize-contrast}"]})};export{B as ImageSegmentationComponent};
